[logging]
identifier = Seq2Seq_testworker

[train] #train parameters
epoch = 12
batch_size = 512

reader_num = 1

[eval] #eval parameters
batch_size = 128
reader_num = 1

[data] #data parameters

train_formatter_type = SeqFormatter
valid_formatter_type = SeqFormatter
test_formatter_type = SeqFormatter


[model] #model parameters
model_name = Seq2Seq

src_vocab_path = /home/dkb/workspace/Code/nlp_data/jieba/src_dic
trg_vocab_path = /home/dkb/workspace/Code/nlp_data/jieba/trg_level_3.dic
eval_vocab_path = /home/dkb/workspace/Code/nlp_data/jieba/trg_label.dic

emb_size = 100
hidden_size = 256

num_layers = 1

attention = luong_gate
mask = True
length_norm = True
beam_size = 4
global_emb = False
max_time_step = 6
bidirec = True
dropout = 0.1
tau: 0.1
pool_size = 0

[optim] #optimizer and lr_scheduler parameters
optimizer = adam
learning_rate =  0.0006
weight_decay = 0

lr_scheduler = Step
step_size = 1
lr_multiplier = 1
update_scheduler = step

[output] #output parameters
output_time = 200
model_name = Seq2Seq

[report] #report parameters

report_fun = report_Basic_F1
metric = macro_f1