[logging]
identifier = Transformer

[train] #train parameters
epoch = 13
batch_size = 128

reader_num = 5


[eval] #eval parameters
batch_size = 128
reader_num = 2

[data] #data parameters

train_formatter_type = TransFormatter
valid_formatter_type = TransFormatter
test_formatter_type = TransFormatter

max_seq_length = 200


[model] #model parameters
model_name = Transformer
Beam_Search = False

max_time_step = 4

model_dim = 256

ff_dim = 512

n_layers = 3

n_head = 8

dropout = 0.1

max_len = 4

src_vocab_path = /home/dkb/workspace/Code/nlp_data/jieba/src_dic
trg_vocab_path = /home/dkb/workspace/Code/nlp_data/jieba/trg_level_3.dic
eval_vocab_path = /home/dkb/workspace/Code/nlp_data/jieba/trg_label.dic

[output] #output parameters

output_time = 200
model_name = Transformer
output_function = seq_index

[optim] #optimizer and lr_scheduler parameters
optimizer = transformer
learning_rate =  0

lr_scheduler = Transformer

update_scheduler = step

[report] #report parameters

report_fun = report_Basic_F1
metric = macro_f1