[logging]
identifier = Som_Classifier

[train] #train parameters
epoch = 2
batch_size = 32
reader_num = 0



[eval] #eval parameters
batch_size = 64
reader_num = 0

[data] #data parameters


max_seq_length = 300

train_data_path = /home/dkb/workspace/Code/nlp_data/jieba/test.json
valid_data_path = /home/dkb/workspace/Code/nlp_data/jieba/valid.json

train_formatter_type = SomFormatter
valid_formatter_type = SomFormatter
test_formatter_type = SomFormatter


[model] #model parameters
model_name = somdst
bert_path =/home/dkb/workspace/Code/Code/Bert_path/bert-base-chinese
eval_vocab_path = /home/dkb/workspace/Code/nlp_data/Bert/merge.dic

n_domain = 8
exclude_domain = False
update_id = 1
n_op = 1
dropout = 0.1
hidden_size = 256
word_dropout = 0.1



[output] #output parameters

eval_output = Test_eval
output_time = 10
model_name = Som

[optim] #optimizer and lr_scheduler parameters
optimizer = Som
learning_rate =  0

enc_lr = 0.00004
dec_lr = 0.0003
warmup_rate = 0.1

lr_scheduler = Som

update_scheduler = step

[report] #report parameters

report_fun = report_Som
metric = macro_f1

