[logging]
identifier = MatchLSTM_testworker

[train] #train parameters
epoch = 13
batch_size = 128

reader_num = 5

[eval] #eval parameters
batch_size = 128
reader_num = 2


[data] #data parameters

train_formatter_type = Match_Formatter
valid_formatter_type = Match_Formatter
test_formatter_type = Match_Formatter

[model] #model parameters
model_name = ParMatchLSTM

src_vocab_path = /home/dkb/workspace/Code/nlp_data/jieba/src_label.dic
trg_vocab_path = /home/dkb/workspace/Code/nlp_data/dummy/merge.dic
eval_vocab_path = /home/dkb/workspace/Code/nlp_data/dummy/merge.dic

re_trg_vocab_path = /home/dkb/workspace/Code/nlp_data/dummy/re_trg_3.dic
dummy_vocab_path = /home/dkb/workspace/Code/nlp_data/dummy/dum.dic

emb_size = 100
hidden_size = 256

num_classes = 72
n_class_dummy_1 = 4
n_class_dummy_2 = 9
n_class_dummy_3 = 13
n_class_dummy = 26
n_class_retrg = 49

use_gru = False
num_layers = 1
model_dropout = 0.2

dropout = 0.5

[optim] #optimizer and lr_scheduler parameters
optimizer = adam
learning_rate =  0
weight_decay = 0

lr_scheduler = Step
step_size = 1
lr_multiplier = 1
update_scheduler = step


[output] #output parameters

output_time = 200
model_name = ParMatchLSTM
threshold = 0.2
top-k = 3

[report] #report parameters

report_fun = report_Basic_F1
metric = macro_f1